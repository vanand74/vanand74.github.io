var abstracts = {
    "ABL1":`<h2>Obtaining Informationally-Consistent Decisions when Computing Costs with Limited Information</h2><p>We demonstrate the need to view in a dynamic context any decision based on limited information. We focus on the use of product costs in selecting the product portfolio. We show how ex post data regarding the actual costs from implementing the decision leads to updating of product cost estimates and potentially trigger a revision of the initial decision. We model this updating process as a discrete dynamical system (DDS). We define a decision as informationally consistent if it is a fixed-point solution to the DDS. We employ numerical analysis to characterize the existence and properties of such solutions. We find that fixed points are rare, but that simple heuristics find them often and quickly. We demonstrate the usefulness and robustness of our methodology by examining the interaction of limited information with multiple decision rules (heuristics) and problem features (size of product portfolio, profitability of product markets). We discuss implications for research on cost systems. </p>`,
    "ABL_JMAR":`<h2>A Framework for Conducting Numerical Experiments on Cost System Design</h2><p>This paper aims to advance the use of numerical experiments to investigate issues that surround the design of cost systems. As with laboratory and field experiments, researchers must decide on the independent variables and their levels, the experimental design, and the dependent variables. Options for dependent and independent variables are ample, as are the ways in which we can model the relations among these variables. We provide a modular framework that provides structure to these variables, their definitions, and the modeling of the connections among them. Further, we offer some insights into the design and layout of output data files, which will allow for easier data analysis. We also present tips on how to report the results from such numerical experiments effectively. Finally, we furnish online the source code in C# for many of these modules. We hope that the framework and guidance provided in this paper will help spur and focus further meaningful work in this important area of management accounting.</p>`,
    "DP1":`<h2>​Performance Standard Horizons, Uncertainty, and Effort Provision</h2><p>​The horizon of a performance standard is the time given to achieve the standard. Performance standard horizons vary considerably in practice, and the goal-setting literature provides mixed evidence on whether short or long horizons are more effective at eliciting effort from workers. I predict and find that uncertainty in workers’ effort-performance relationship moderates the effect of horizon on effort. Horizon and uncertainty affect expectancy (probability of standard attainment) which in turn determines effort. In an experiment in which the rate of effort needed to achieve the standard was held constant, participants facing low uncertainty decreased effort as I increased horizon. However, those facing high uncertainty increased effort as I increased horizon. These findings are consistent with the expectancy-based predictions, and suggest that managers should jointly determine the level and horizon of performance standards. These findings also provide potential explanations for the variation in horizons observed in practice and for the mixed findings in the goal-setting literature.</p>`,
    "AWW":`<h2>Mitigating the Demotivating Effects of Frequent Unfavorable Feedback About Goal Progress</h2><p>The use of performance goals by organizations to motivate individual effort is pervasive and feedback about goal progress is often available on a highly frequent basis. While research shows feedback can be beneficial, there is evidence that frequent provision of feedback can be demotivating when it indicates unfavorable progress towards goal attainment. We use expectancy theory to predict that compared to infrequent feedback, frequent unfavorable feedback about goal progress will reduce effort by negatively impacting individuals’ expectancy of goal attainment. We also predict that the negative effects of frequent unfavorable feedback on effort will be mitigated when accompanied by a reminder about the attainability of the goal that serves to bolster the expectancy of goal attainment. Results from two experiments support both predictions and also show that provision of a goal attainability reminder does not lead to effort reductions when early frequent feedback is favorable. Our findings have implications for practice in showing that a simple and readily implementable approach of reminding individuals about the attainability of assigned goals can mitigate the potentially negative motivational effects of frequent unfavorable performance feedback.</p>`,
    "ABL2":`<h2>Updating Cost Systems</h2><p>​A cost system provides a snapshot of a firm’s resource consumption by cost objects. However, the firm’s operating environment and the firm itself change over time, affecting the costs to be allocated and resource consumption patterns. A firm may wish to update its cost system to capture these changes. We define various levels of thoroughness in cost system updating, and study the benefits that result when choosing a product portfolio based on data from the cost system. We find that the benefits of partial updates (e.g., updating overhead rates) are limited, and on average result in a reduction in profit relative to not changing the system at all. Further, implementing a sophisticated cost system in the first place results in higher profit than a thorough update later. Lastly, the benefits of a thorough update are highest when the impact of the change in the environment is the greatest.</p>`,
    "ABIS":`<h2>Predicting Profitability Using Machine Learning</h2><p>Out-of-sample prediction of profitability is a critical step in fundamental analysis and yet even sophisticated regression models do not generate predictions that significantly outperform random walk predictions. We employ random forests with classification trees, a method from machine learning, to generate out-of-sample predictions of directional changes (increases or decreases) in five profitability measures, return on equity (ROE), return on assets (ROA), return on net operating assets (RNOA), cash flow from operations (CFO), and free cash flow (FCF). With a minimum set of independent variables, and out-of-sample, our method achieves classification accuracies ranging from 57 – 63% for our profitability measures, compared to 50% for the random walk. The difference in proportions of accurate classifications is highly significant. Out-of-sample classification accuracy is similar over forecast horizons of 1 to 5 years. We observe better performance on cash flow measures than on traditional, earnings-based profitability measures. We find predictive accuracy is highest for firms with high and low accruals-to-market and earnings-to-market ratios, exceeding 75% in one instance. Importantly, our method is insensitive to outliers; our method used data that had not been winsorized or standardized. These results suggest that machine learning methods offer better predictive performance than traditional regression-based methods.</p>`,
    "Francisco": `<h2>Beyond Parameter Estimation: Extending Biomechanical Modeling by the Explicit Exploration of Model Topology</h2><p>Selecting a model topology that realistically predicts biomechanical function remains an unsolved problem. Today's dominant modeling approach is to replicate experimental input/output data by performing parameter estimation on an assumed topology. In contrast, we propose that modeling some complex biomechanical systems requires the explicit and simultaneous exploration of model topology (i.e., the type, number, and organization of physics-based functional building blocks) and parameter values. In this paper, we use the example of modeling the notoriously complex tendon networks of the fingers to present three critical advances towards the goal of implementing this extended modeling paradigm. First, we describe a novel computational environment to perform quasi-static simulations of arbitrary topologies of elastic structures undergoing large deformations. Second, we use this form of simulation to show that the assumed topology for the tendon network of a finger plays an important role in the propagation of tension to the finger joints. Third, we demonstrate the use of a novel inference algorithm that simultaneously explores the topology and parameter values for hidden synthetic tendon networks. We conclude by discussing critical issues of observability, separability, and uniqueness of topological features inferred from input/output data, and outline the challenges that need to be overcome to apply this novel modeling paradigm to extract causal models in real anatomical systems.</p>`,
    "FTA_Python": `<h2>Using Python for Text Analysis in Accounting Research</h2><p>The prominence of textual data in accounting research has increased dramatically. To assist researchers in understanding and using textual data, this monograph defines and describes common measures of textual data and then demonstrates the collection and processing of textual data using the Python programming language. The monograph is replete with sample code that replicates textual analysis tasks from recent research papers.</p><p>In the first part of the monograph, we provide guidance on getting started in Python. We first describe Anaconda, a distribution of Python that provides the requisite libraries for textual analysis, and its installation. We then introduce the Jupyter notebook, a programming environment that improves research workflows and promotes replicable research. Next, we teach the basics of Python programming and demonstrate the basics of working with tabular data in the Pandas package.</p><p>The second part of the monograph focuses on specific textual analysis methods and techniques commonly used in accounting research. We first introduce regular expressions, a sophisticated language for finding patterns in text. We then show how to use regular expressions to extract specific parts from text. Next, we introduce the idea of transforming text data (unstructured data) into numerical measures representing variables of interest (structured data). Specifically, we introduce dictionary-based methods of 1) measuring document sentiment, 2) computing text complexity, 3) identifying forward-looking sentences and risk disclosures, 4) collecting informative numbers in text, and 5) computing the similarity of different pieces of text. For each of these tasks, we cite relevant papers and provide code snippets to implement the relevant metrics from these papers.</p><p>Finally, the third part of the monograph focuses on automating the collection of textual data. We introduce web scraping and provide code for downloading filings from EDGAR.</p>`,
    "CP": `<h2>Capacity Planning with Limited Information</h2><p>Limited information about the demand for some of the resources needed to produce goods and services (e.g., incomplete and imperfect bills of materials) forces firms to use heuristics when planning resource ca-pacity. We examine the performance of five heuristics: two drawn from practice, two that modify observed approaches, and one motivated by theory. We measure performance as the ratio of the expected cost of supply-demand mismatch from using a heuristic to the value in the full-information solution. Numerical analysis shows that a simple heuristic that is common in practice – plan rigorously for a few “driver” re-sources with high-quality information and use ratios (e.g., 0.25 indirect labor hours per machine hour) to project the capacities for the remaining “non-driver” resources – is robust and efficient. Using more than one driver resource to plan for the same non-driver resource delivers significant gains. Reducing measure-ment error with respect to the consumption of driver resources dominates the gain from reducing errors on other aspects. Indeed, with high measurement error, collecting information that reduces other sources of error could decrease overall performance. Finally, a greedy algorithm of choosing the most expensive re-sources as drivers is optimal.</p>`
}